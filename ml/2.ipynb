{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.pyplot.style.use('seaborn')\n",
    "matplotlib.rcParams['figure.figsize'] = (15, 5)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=2, suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import copy\n",
    "\n",
    "import scipy.stats as stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection, metrics, datasets\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "# $Model(x_i, w) = w_0 + \\sum_j^d w_j x_i^j$\n",
    "\n",
    "$x_i$ - $i'th$ featrue in matrix of features $X$\n",
    "\n",
    "$x_i^j$ - $j'th$ featrue value of current feature $i$\n",
    "\n",
    "$w_j$ - weight for $x_i^j$ feature value\n",
    "\n",
    "$w_0$ - free coeficent\n",
    "\n",
    "---\n",
    "\n",
    "In matrix (add $x_i^0$ and set it to $1$, and add $w_0$ for it)\n",
    "\n",
    "$Model(X, w) = Xw$\n",
    "\n",
    "$\\begin{bmatrix}\n",
    "(x_0^0=1), & ..., & (x_0^j) \\\\\n",
    "(x_1^0=1), & ..., & (x_1^j) \\\\\n",
    "... \\\\\n",
    "(x_n^0=1), & ..., & (x_n^j) \\\\\n",
    "\\end{bmatrix}$\n",
    "$\\begin{bmatrix}\n",
    "w_0 \\\\\n",
    "...\\\\\n",
    "w_j\n",
    "\\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = np.array([\n",
    "    [1, 2, 5],\n",
    "    [1, 5, 5],\n",
    "    [1, 8, 5],\n",
    "], dtype=np.float64)\n",
    "\n",
    "w = np.array([0.1, 0.1, 0.1], dtype=np.float64)\n",
    "\n",
    "yhat = X.dot(w)\n",
    "yhat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### $y = Xw + \\epsilon$\n",
    "\n",
    "$\\begin{bmatrix}\n",
    "y_0 \\\\\n",
    "... \\\\\n",
    "y_n \\\\\n",
    "\\end{bmatrix} = $\n",
    "$\\begin{bmatrix}\n",
    "x_0^0 & x_0^1 &  ... & x_0^j \\\\\n",
    "... \\\\\n",
    "x_n^0 & x_n^1 &  ... & x_n^j\n",
    "\\end{bmatrix}$\n",
    "$\\begin{bmatrix}\n",
    "w_0 \\\\\n",
    "...\\\\\n",
    "w_j\n",
    "\\end{bmatrix} +$\n",
    "$\\begin{bmatrix}\n",
    "\\epsilon_0 \\\\\n",
    "...\\\\\n",
    "\\epsilon_n\n",
    "\\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification\n",
    "\n",
    "### Link function: squeeze regression real line into $[0,1]$ (and make it probability)\n",
    "\n",
    "$P(y=1|X, w) = Sigmoid(Model(X, w))$\n",
    "\n",
    "---\n",
    "\n",
    "### Logistic function (sigmoid, logit)\n",
    "\n",
    "### $Sigmoid(X, w) = \\frac{1}{1 + e^{-Model(X,w)}}$\n",
    "\n",
    "$Sigmoid(X, w) = \\frac{1}{1 + e^{-(w_0 + \\sum_j^d w_j x_i^j)}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x, w=1):\n",
    "    # w - coeff weights is a constant to a function, makin' function tighter, also makes confidence borders lower\n",
    "    return 1 / (1 + np.exp(-x*w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = np.linspace(-5, 5, 100)\n",
    "ys = [sigmoid(x) for x in xs]\n",
    "plt.plot(xs, ys);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([\n",
    "    [1, 2, 5],\n",
    "    [1, 5, 5],\n",
    "    [1, 8, 5],\n",
    "], dtype=np.float64)\n",
    "\n",
    "w = np.array([0.1, 0.1, 0.1], dtype=np.float64)\n",
    "\n",
    "proba = sigmoid(X.dot(w))\n",
    "proba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### maximum likelihood estimation (MLE) \n",
    "\n",
    "### $\\ell(w) = \\prod_i^N P(y_i | x_i, w)$\n",
    "\n",
    "### $\\ell\\ell(w) = \\ln\\prod_i^N P(y_i | x_i, w)$\n",
    "\n",
    "### $\\ell\\ell(w) \\rightarrow \\underset{w}{max}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute derivative of log likelihood with respect to a single coefficient\n",
    "\n",
    "$\\frac{\\partial\\ell}{\\partial w_j} = \\sum_{i=1}^N\\Big(\n",
    "x_i \\cdot \\left([y_i == 1] - P(y_i = 1 | \\mathbf{x}_i, \\mathbf{w})\\right)\n",
    "\\Big)$\n",
    "\n",
    "The log likelihood: \n",
    "\n",
    "$\\frac{\\partial}{\\partial w_j} \\ell\\ell(\\mathbf{w}) = \\sum_{i=1}^N \\Big( ([y_i == 1] - 1)\\mathbf{w}^T x_i - \\ln\\left(1 + e^{-\\mathbf{w}^T x_i}\\right) \\Big) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### * * * derivation\n",
    "\n",
    "$\\ell(w) = \\prod_i^N P(y_i | x_i, w)$\n",
    "\n",
    "log turns into sumation and does not change maximum optima:\n",
    "\n",
    "$\\ell\\ell(w) = \\ln\\prod_i^N P(y_i | x_i, w)$\n",
    "\n",
    "$\\ln\\prod_i^N P(y_i | x_i, w) \\rightarrow \\sum_i^N \\ln P(y_i | x_i, w)$\n",
    "\n",
    "---\n",
    "\n",
    "$\\sum_i^N \\ln P(y_i | x_i, w)$\n",
    "\n",
    "$\\sum_i^N\\left(\n",
    "[y_i == 1] \\ln P(y_i = 1 | x_i, w) +\n",
    "[y_i == 0] \\ln P(y_i = 0 | x_i, w)\n",
    "\\right)$\n",
    "\n",
    "$P(y_i = 1 | x_i, w) = \\frac{1}{1 + e^{-w^Tx}}$\n",
    "\n",
    "$P(y_i = 0 | x_i, w) = \\frac{e^{-w^Tx}}{1 + e^{-w^Tx}}$\n",
    "\n",
    "$\\sum_i^N\\left(\n",
    "[y_i == 1] \\ln \\frac{1}{1 + e^{-w^Tx}} +\n",
    "[y_i == 0] \\ln  \\frac{e^{-w^Tx}}{1 + e^{-w^Tx}}\n",
    "\\right)$\n",
    "\n",
    "$\\sum_{i=1}^N \\Big( -(1 - [y_i == 1])\\mathbf{w}^T x_i - \\ln\\left(1 + e^{-\\mathbf{w}^T x_i}\\right) \\Big)$\n",
    "\n",
    "$\\sum_{i=1}^N \\Big( ([y_i == 1] - 1)\\mathbf{w}^T x_i - \\ln\\left(1 + e^{-\\mathbf{w}^T x_i}\\right) \\Big)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Ascent (MLE)\n",
    "\n",
    "$w^t = w^{t-1} + \\alpha_t \\triangledown \\ell\\ell(w^{t-1},X,y) - \\lambda_t \\left\\| w \\right\\|_2^2$\n",
    "\n",
    "$\\triangledown$ - gradient _(vector of derivatives)_\n",
    "\n",
    "$\\alpha_t$ - learning rate for current step\n",
    "\n",
    "$\\lambda \\left\\| w \\right\\|_2^2$ - l2 regularization _(square of second norm)_\n",
    "\n",
    "$t$ - iteration\n",
    "\n",
    "Stop: $\\left\\| w_t - w^{t-1} \\right\\| < \\epsilon$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(w, X):\n",
    "    return 1 / (1 + np.exp(-w.dot(X.T)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mle(w, X):\n",
    "    return np.log(np.prod(sigmoid(w, X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gmle(w, X, y):\n",
    "    indicator = y - 1 # $[y_i == 1] - 1$\n",
    "    return (indicator * w.dot(X.T)) - np.log(1 + np.exp(-w.dot(X.T)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge(w, l):\n",
    "    w = w.copy()\n",
    "    w[0] = 0 # Donâ€™t penalize intercept term w0\n",
    "    return 2 * l * w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maximize(X, y, cost, grad, reg):\n",
    "    # add coef of ones\n",
    "    X = np.append(np.ones(len(X[0])), X.T).reshape(4,3).T\n",
    "\n",
    "    # weights vector\n",
    "    w = np.zeros(len(X[0]), dtype=np.float64)\n",
    "\n",
    "    # parameters\n",
    "    epsilon = 0.0\n",
    "    alpha = 0.1\n",
    "    weights = [w]\n",
    "    error = []\n",
    "    \n",
    "    for iteration in range(500):\n",
    "        print(w)\n",
    "        print(grad(w, X, y))\n",
    "        w = w - alpha * grad(w, X, y) - reg(w, 0.00001)\n",
    "        if np.linalg.norm(w - weights[-1]) < epsilon:\n",
    "            break\n",
    "        weights.append(w)\n",
    "        error.append(cost(w, X))\n",
    "\n",
    "    return w[0], w[1:], error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, w, coef):\n",
    "    X = np.append(np.ones(len(X[0])), X.T).reshape(4,3).T\n",
    "    w = np.append(np.array(coef), w)\n",
    "\n",
    "    return X.dot(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([\n",
    "    [0.2, 0.15, 0.8],\n",
    "    [0.5, 0.45, 0.12],\n",
    "    [0.8, 0.53, 0.33],\n",
    "], dtype=np.float64)\n",
    "\n",
    "y = np.array([\n",
    "    0,\n",
    "    1,\n",
    "    0,\n",
    "], dtype=np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef, w, error = maximize(X, y, mle, gmle, ridge)\n",
    "\n",
    "coef\n",
    "\n",
    "w\n",
    "predict(X, w, coef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(error);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check by sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "reg = LogisticRegression().fit(X, y)\n",
    "\n",
    "reg.intercept_ \n",
    "\n",
    "reg.coef_\n",
    "reg.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
